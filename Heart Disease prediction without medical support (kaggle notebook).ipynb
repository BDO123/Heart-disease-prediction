{"cells":[{"metadata":{"_uuid":"a5d64a01cced5bd322352a178e149214d5fc2e65"},"cell_type":"markdown","source":"* Budhdeo Kumar\n* Information Technology Department\n* Birla Institute of Technology , Patna\n* budhdeo.abc@gmail.com\n* JAN 31 , 2019"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import environment tools\nimport re\nimport itertools\nimport warnings\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport keras\n\n# Import plotly tools\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Import keras tools\nfrom keras import regularizers\nfrom keras.callbacks import History \nfrom keras.layers import Dense, Input, Dropout\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Import other tools\nfrom __future__ import print_function\nfrom pandas import read_excel\nfrom IPython.display import Image\nfrom collections import Counter\nfrom itertools import cycle\nfrom scipy import stats, integrate, interp\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"import sklearn\n# Gradient Boosters\nimport xgboost as xgb # Accuracy\nimport lightgbm as lgb # Speed\n\nfrom sklearn import decomposition, preprocessing, svm\n# Dimensionality Reduction\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n# Ensemble\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n# Guassian\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n# Regression\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n# Bayesian\nfrom sklearn.naive_bayes import GaussianNB\n# Instance Based\nfrom sklearn.neighbors import KNeighborsClassifier\n# Nueral Network\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import relevant machine learning analyis tools\nfrom sklearn import metrics\n#from sklearn.cross_validation import KFold, train_test_split\n# Imputation\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.metrics import mean_absolute_error,roc_curve,accuracy_score,auc,roc_auc_score,confusion_matrix,precision_score,recall_score,f1_score, classification_report\nfrom sklearn.metrics.cluster import fowlkes_mallows_score\nfrom sklearn.model_selection import BaseCrossValidator, GridSearchCV, train_test_split,cross_val_score,cross_validate,cross_val_predict, KFold, StratifiedKFold, learning_curve\nfrom sklearn.pipeline import Pipeline\n# Standardization\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n\n\n# Initial tool settings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nplt.style.use('fivethirtyeight')\nsns.set(style='white', context='notebook', palette='deep')\npy.init_notebook_mode(connected=True)\n\nhistory = History()\nrandom_state = 43\n\nnames = [\"k-Nearest Neighbors\",         \n         \"Support Vector Machine\",\n         \"Linear SVM\",\n         \"RBF SVM\",\n         \"Gaussian Process\",\n         \"Decision Tree\",\n         \"Extra Trees\",\n         \"Random Forest\",\n         \"Extra Forest\",\n         \"AdaBoost\",\n         \"Gaussian Naive Bayes\",\n         \"LDA\",\n         \"QDA\",\n         \"Logistic Regression\",\n         \"SGD Classifier\",\n         \"Multilayer Perceptron\",\n         \"Voting Classifier\"\n        ]\n\nalgorithms = [ KNeighborsClassifier(n_neighbors=3),\n               SVC(random_state=random_state),\n               SVC(kernel=\"linear\",random_state=random_state),\n               SVC(kernel=\"rbf\",random_state=random_state),\n               GaussianProcessClassifier(),\n               DecisionTreeClassifier(random_state=random_state),\n               ExtraTreesClassifier(random_state=random_state),\n               RandomForestClassifier(random_state=random_state),\n               GradientBoostingClassifier(random_state=random_state),\n               AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),n_estimators=10,learning_rate=0.1,random_state=random_state),\n               GaussianNB(),\n               LinearDiscriminantAnalysis(),\n               QuadraticDiscriminantAnalysis(),\n               LogisticRegression(random_state=random_state),\n               SGDClassifier(),               \n               MLPClassifier(hidden_layer_sizes=(100,),momentum=0.9,solver='sgd',random_state=random_state),\n               VotingClassifier(estimators=[('log', LogisticRegression()), ('SVM',SVC(C=1000)), ('MLP', MLPClassifier(hidden_layer_sizes=(100,)))], voting='hard')\n              ]\n#algorithms.append(SVC(random_state=random_state))\n\nclassifiers = {  \"k-Nearest Neighbors\" : KNeighborsClassifier(n_neighbors=3),\n                 \"Support Vector Machine\" :  SVC(random_state=random_state),\n                 \"Linear SVM\" :  SVC(kernel=\"linear\",random_state=random_state),\n                 \"RBF SVM\" :  SVC(kernel=\"rbf\",random_state=random_state),\n                 \"Gaussian Process\" : GaussianProcessClassifier(),\n                 \"Decision Tree\" : DecisionTreeClassifier(random_state=random_state),\n                 \"Extra Trees\" : ExtraTreesClassifier(random_state=random_state),\n                 \"Random Forest\" : RandomForestClassifier(random_state=random_state),\n                 \"Extra Forest\" : GradientBoostingClassifier(random_state=random_state),\n                 \"AdaBoost\" : AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),n_estimators=10,random_state=random_state,learning_rate=0.1),\n                 \"Gaussian Naive Bayes\" : GaussianNB(),\n                 \"LDA\" : LinearDiscriminantAnalysis(),\n                 \"QDA\" :  QuadraticDiscriminantAnalysis(),\n                 \"Logistic Regression\" : LogisticRegression(random_state=random_state),\n                 \"SGD Classifier\" : SGDClassifier(),\n                 \"Multilayer Perceptron\" :  MLPClassifier(hidden_layer_sizes=(100,),momentum=0.9,solver='sgd',random_state=random_state),\n                 \"Voting Classifier\" : VotingClassifier(estimators=[('log', LogisticRegression()), ('SVM',SVC(C=1000)), ('MLP', MLPClassifier(hidden_layer_sizes=(100,)))], voting='hard')\n              }\nprint(\"Done!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import Data**"},{"metadata":{"trusted":true,"_uuid":"e757b24fe75badd1131a38fa00243e0e013c8447"},"cell_type":"code","source":"df=pd.read_csv(\"../input/heart-disease-with-76-attributes/cleveland_data.csv\",header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Cleaning**\n* Milestones in data cleaning:\n1. 1.  Information of each person is seperated in 3 or 4 or 5 rows.\n1. 2.  Missing values\n1. 3.  Untidy Dataset"},{"metadata":{},"cell_type":"markdown","source":"**Conversion of datatype of every data for data cleaning**\n* Change each data to string dtype "},{"metadata":{"trusted":true,"_uuid":"4ce3b4333a636bb8cf218c4dc3b0e3996886b6b7"},"cell_type":"code","source":"rw=[]\nfor row_index,row in df.iterrows():\n    x=str(row)\n    rw.append(list(map(str,x.split())))\nrow1=list(df[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A Python function to parse and extract the data of 76 columns for each row and select only those rows which have every data for 76 columns."},{"metadata":{"trusted":true,"_uuid":"ffba595989bb3ffc66393620c5487aa7b34770d6"},"cell_type":"code","source":"row1=list(df[0])\nrow1[0]='1 0 63 1 9 9 9'\nrow2=[]\ns=''\nfor i in row1:\n    if(i[-1]=='e'):\n        s=s+\" \"+i\n        row2.append(s)\n        s=''\n    else:\n        s=s+\" \"+i\nfor i in range(len(row2)):\n    row2[i]=list(row2[i].split())\nfault=[]\nfor i in range(len(row2)):\n    if(len(row2[i]) != 76):\n        fault.append(i)\nrow3=row2[:282]\ndata = pd.DataFrame(row3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"85bba5b7fcf7522d9f2a5b928e1cfb8f1501015f"},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**76 features in dataset**\n* Complete attribute documentation: \n* 1 id: patient identification number \n* 2 ccf: social security number (I replaced this with a dummy value of 0) \n* 3 age: age in years \n* 4 sex: sex (1 = male; 0 = female) \n* 5 painloc: chest pain location (1 = substernal; 0 = otherwise) \n* 6 painexer (1 = provoked by exertion; 0 = otherwise) \n* 7 relrest (1 = relieved after rest; 0 = otherwise) \n* 8 pncaden (sum of 5, 6, and 7) \n* 9 cp: chest pain type \n* -- Value 1: typical angina \n* -- Value 2: atypical angina \n* -- Value 3: non-anginal pain \n* -- Value 4: asymptomatic \n* 10 trestbps: resting blood pressure (in mm Hg on admission to the hospital) \n* 11 htn \n* 12 chol: serum cholestoral in mg/dl \n* 13 smoke: I believe this is 1 = yes; 0 = no (is or is not a smoker) \n* 14 cigs (cigarettes per day) \n* 15 years (number of years as a smoker) \n* 16 fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n* 17 dm (1 = history of diabetes; 0 = no such history) \n* 18 famhist: family history of coronary artery disease (1 = yes; 0 = no) \n* 19 restecg: resting electrocardiographic results \n* -- Value 0: normal \n* -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n* -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n* 20 ekgmo (month of exercise ECG reading) \n* 21 ekgday(day of exercise ECG reading) \n* 22 ekgyr (year of exercise ECG reading) \n* 23 dig (digitalis used furing exercise ECG: 1 = yes; 0 = no) \n* 24 prop (Beta blocker used during exercise ECG: 1 = yes; 0 = no) \n* 25 nitr (nitrates used during exercise ECG: 1 = yes; 0 = no) \n* 26 pro (calcium channel blocker used during exercise ECG: 1 = yes; 0 = no) \n* 27 diuretic (diuretic used used during exercise ECG: 1 = yes; 0 = no) \n* 28 proto: exercise protocol \n* 1 = Bruce \n* 2 = Kottus \n* 3 = McHenry \n* 4 = fast Balke \n* 5 = Balke \n* 6 = Noughton \n* 7 = bike 150 kpa min/min (Not sure if \"kpa min/min\" is what was written!) \n* 8 = bike 125 kpa min/min \n* 9 = bike 100 kpa min/min \n* 10 = bike 75 kpa min/min \n* 11 = bike 50 kpa min/min \n* 12 = arm ergometer \n* 29 thaldur: duration of exercise test in minutes \n* 30 thaltime: time when ST measure depression was noted \n* 31 met: mets achieved \n* 32 thalach: maximum heart rate achieved \n* 33 thalrest: resting heart rate \n* 34 tpeakbps: peak exercise blood pressure (first of 2 parts) \n* 35 tpeakbpd: peak exercise blood pressure (second of 2 parts) \n* 36 dummy \n* 37 trestbpd: resting blood pressure \n* 38 exang: exercise induced angina (1 = yes; 0 = no) \n* 39 xhypo: (1 = yes; 0 = no) \n* 40 oldpeak = ST depression induced by exercise relative to rest \n* 41 slope: the slope of the peak exercise ST segment \n* -- Value 1: upsloping \n* -- Value 2: flat \n* -- Value 3: downsloping \n* 42 rldv5: height at rest \n* 43 rldv5e: height at peak exercise \n* 44 ca: number of major vessels (0-3) colored by flourosopy \n* 45 restckm: irrelevant \n* 46 exerckm: irrelevant \n* 47 restef: rest raidonuclid (sp?) ejection fraction \n* 48 restwm: rest wall (sp?) motion abnormality \n* 0 = none \n* 1 = mild or moderate \n* 2 = moderate or severe \n* 3 = akinesis or dyskmem (sp?) \n* 49 exeref: exercise radinalid (sp?) ejection fraction \n* 50 exerwm: exercise wall (sp?) motion \n* 51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n* 52 thalsev: not used \n* 53 thalpul: not used \n* 54 earlobe: not used \n* 55 cmo: month of cardiac cath (sp?) (perhaps \"call\") \n* 56 cday: day of cardiac cath (sp?) \n* 57 cyr: year of cardiac cath (sp?) \n* 58 num: diagnosis of heart disease (angiographic disease status) \n* -- Value 0: < 50% diameter narrowing \n* -- Value 1: > 50% diameter narrowing \n* (in any major vessel: attributes 59 through 68 are vessels) \n* 59 lmt \n* 60 ladprox \n* 61 laddist \n* 62 diag \n* 63 cxmain \n* 64 ramus \n* 65 om1 \n* 66 om2 \n* 67 rcaprox \n* 68 rcadist \n* 69 lvx1: not used \n* 70 lvx2: not used \n* 71 lvx3: not used \n* 72 lvx4: not used \n* 73 lvf: not used \n* 74 cathef: not used \n* 75 junk: not used \n* 76 name: last name of patient (I replaced this with the dummy string \"name\")\n\n"},{"metadata":{"trusted":true,"_uuid":"b0d792ad3be0efeb163f5e570aa71d9716cd5796"},"cell_type":"code","source":"features=['id', 'ccf', 'age', 'sex', 'painloc', 'painexer', 'relrest', 'pncaden', 'cp', 'trestbps', 'htn', 'chol','smoke', 'cigs', 'years', 'fbs', 'dm', 'famhist', 'restecg', 'ekgmo', 'ekgday', 'ekgyr', 'dig', 'prop', 'nitr', 'pro', 'diuretic', 'proto', 'thaldur', 'thaltime', 'met', 'thalach', 'thalrest', 'tpeakbps', 'tpeakbpd', 'dummy', 'trestbpd', 'exang', 'xhypo', 'oldpeak', 'slope', 'rldv5', 'rldv5e', 'ca', 'restckm', 'exerckm', 'restef', 'restwm', 'exeref', 'exerwm', 'thal', 'thalsev', 'thalpul', 'earlobe', 'cmo', 'cday', 'cyr', 'pred_attribute', 'lmt', 'ladprox', 'laddist', 'diag', 'cxmain', 'ramus', 'om1', 'om2', 'rcaprox', 'rcadist', 'lvx1', 'lvx2', 'lvx3', 'lvx4', 'lvf', 'cathef', 'junk', 'name']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting up dataset with features as column name."},{"metadata":{"trusted":true,"_uuid":"8a06d01bd56d25f3556621165c9fa09b65c74a91"},"cell_type":"code","source":"data.columns=features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a151faeeccb974188b57cb7c72a233c6c26e34e4"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**"},{"metadata":{"trusted":true,"_uuid":"4cd77a5cebc0275dead81c840982086863b5b20f"},"cell_type":"code","source":"sns.countplot(x=data['dm'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows that there is missing data in history of diabities in their family \"dm\" column which is non-medical attribute."},{"metadata":{"trusted":true,"_uuid":"3077dad6c877dd3baeaaccf1e1bfb6be0c0e89c3"},"cell_type":"code","source":"r=list(data['cigs'])\nl=list(data['smoke'])\nfor i in range(len(r)):\n    if(int(r[i])>0):\n        l[i]=1\n    else:\n        l[i]=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These attributes are used for further model."},{"metadata":{"trusted":true,"_uuid":"84c0b236b4f60241f54c5cc16d40b07b70a8cb71"},"cell_type":"code","source":"attributes_used=['age','sex','cp','thalach','fbs','famhist','thalrest','trestbpd','exang','pred_attribute']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e631dc740cd47d8e25023dd670913c39bcebb14d"},"cell_type":"code","source":"m=[]\nfor i in range(len(features)):\n    if(features[i] in attributes_used):\n        pass\n    else:\n        m.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the unused columns."},{"metadata":{"trusted":true,"_uuid":"66df2cd4cd9fc4a81fb0df665e537db2ffeb9ea4"},"cell_type":"code","source":"data.drop(data.columns[m], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"258533088b553c99adbd3d91aa5a542a93b58e7f"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a163a7540685329231c375af42081cda0c2788f2"},"cell_type":"code","source":"data = data.convert_objects(convert_numeric=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a0862ff4e67a0c9bb08b05286ea556568e64376"},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c2c33cba466137d4f0c3cb7b20371db3baee01"},"cell_type":"code","source":"data[\"pred_attribute\"].replace(inplace=True, value=[1, 1, 1, 1], to_replace=[1, 2, 3, 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7543d8920f5ffc9a556d921dd90696b4c447a591"},"cell_type":"code","source":"data.sort_values('pred_attribute').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b5d0c150be24c05678c3ac4730d297631188c3a"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of data in each column of attributes used."},{"metadata":{"trusted":true,"_uuid":"020436290029bd654b11f10572b3013c9d851106"},"cell_type":"code","source":"plt.subplots(figsize=(20,15))\nlength=len(attributes_used)\nfor i,j in itertools.zip_longest(attributes_used,range(length)):\n    plt.subplot((length/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    data[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a84f585d57f944e52bc7aa78a315e29c7c69b84"},"cell_type":"code","source":"cont_attributes=['age','thalach','thalrest','trestbpd','pred_attribute']\ndisc_attributes=['cp','fbs','famhist','exang','pred_attribute']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Relation between attributes and prediction attributes."},{"metadata":{"trusted":true,"_uuid":"7d8afa8a637f357c9da8160b714b75040f3aeadd"},"cell_type":"code","source":"sns.pairplot(data=data[cont_attributes],hue='pred_attribute',diag_kind='kde')\n#plt.gcf().set_size_inches(20,15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7200e2739699ae75993d19c271c3a189d585d37"},"cell_type":"code","source":"sns.pairplot(data=data[disc_attributes],hue='pred_attribute',diag_kind='kde')\n#plt.gcf().set_size_inches(20,15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking of outliers."},{"metadata":{"trusted":true,"_uuid":"8027305722abaccde133cb783fa119674fcd89dc"},"cell_type":"code","source":"sns.boxplot(data=data[cont_attributes])\n#plt.gcf().set_size_inches(20,15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdb4bd13ded5cbe2bf5d96825ccfdf58ab532d1d"},"cell_type":"code","source":"data[cont_attributes].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158ca6ca363de0d76293c7ccbd9480e966078bc1"},"cell_type":"code","source":"sns.countplot(x='pred_attribute',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd120d31749d6afa38e771725934ea8db759a662"},"cell_type":"code","source":"sns.countplot(x='sex',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64e5d27afe35f49f2dd7658c074379c7da5bd207"},"cell_type":"code","source":"sns.countplot(x='fbs',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"568fd23400f84de4932b69dd28449bca16833a6e"},"cell_type":"code","source":"#sns.countplot(x='smoke',data=data)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28bf7dc630c831a83c46fc8ca92d191227015904"},"cell_type":"code","source":"sns.countplot(x='cp',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9ecc36207a2e9c0523305f4de61bdb0ecd4f4d5"},"cell_type":"code","source":"sns.countplot(x='famhist',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for null value."},{"metadata":{"trusted":true,"_uuid":"95bd7f60d6faa5a81b9b4e52423c6c73b951d002"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting up for Machine learnig model."},{"metadata":{"trusted":true,"_uuid":"bdb9d777651a2d997af13b36682545bf2afb0eb0"},"cell_type":"code","source":"#dataset = dataset.convert_objects(convert_numeric=True)\n\n# Load data\nX = data.iloc[:, :-1].values  \ny = data.iloc[:, -1].values # = dataset.iloc[:, 12].values\n\n#dataset.dropna(inplace=True, axis=0, how=\"any\")\n#X=dataset.loc[:, \"age\":\"thal\" ]\n#y=dataset[\"pred_attribute\"]\n#np_X = np_dataset[:, :-1]  \n#np_y = np_dataset[:, -1]  \n\nmy_imputer = SimpleImputer()\nmy_imputer = my_imputer.fit(X[:,0:10])   \nX[:, 0:10] = my_imputer.transform(X[:, 0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98870661bdf9e0b0c078d4bf980409b87f6a93c3"},"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n#features_all = dataset[dataset.columns[:13]]\n#features_standard = scaler.fit_transform(dataset[dataset.columns[:13]]) # Gaussian Standardisation\n#X = pd.DataFrame(features_standard,columns=[feature13])\n#X['pred_attribute'] = dataset['pred_attribute']\n#outcome = X['pred_attribute']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting of dataset in train and test ."},{"metadata":{"trusted":true,"_uuid":"abfe681b977d2158ba32941fd77c35ef255b24fa"},"cell_type":"code","source":"# evaluate the model by splitting into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n\nfreqs = pd.DataFrame({\"Training dataset\": y_train.sum(),\n                      \"Test dataset\":y_test.sum(),\n                      \"Total\": y.sum()},\n                     index=[\"Healthy\", \"Sick\"])\nfreqs[[\"Training dataset\", \"Test dataset\", \"Total\"]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ce150971aca8075d612f27c4343620be9941c4f"},"cell_type":"code","source":"def err_score(X_train, X_test, y_train, y_test):\n    model =  RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n    \n    # predict class labels for the train set\n    pred_train = model.predict(X_train)\n    # predict class labels for the test set\n    pred_test = model.predict(X_test)\n    # check the mean absolute error on test set\n    print(\"Mean Absolute Error from imputation: \", mean_absolute_error(y_test, pred_test))\n    \n#X = my_imputer.fit_transform(X)\n#X_train = my_imputer.fit_transform(X_train)\n#X_test = my_imputer.transform(X_test)\nerr_score(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11ac8390cb1a713b07068ca2e5390018a81f2cdf"},"cell_type":"code","source":"# instantiate a logistic regression model, and fit with X and y (with training data in X,y)\nmodel = LogisticRegression(random_state = random_state)\nmodel.fit(X_train, y_train)\n\n# check the accuracy on the training set\nprint(\"Accuracy on training set: \", model.score(X_train, y_train))\n# check the accuracy on the test set\nprint(\"Accuracy on test set: \", model.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf04709208146fe92e868152cd78287f866e48c3"},"cell_type":"code","source":"pred_train = model.predict(X_train)\n#confusion_matrix(y_train,pred_train)\npd.crosstab(y_train, pred_train, rownames=['Predicted'], colnames=['Reality'], margins=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8532e005cb86f410024fcaa55aaaa2e79dc9289f"},"cell_type":"code","source":"pred_test = model.predict(X_test)\n#confusion_matrix(y_test,pred_test)\npd.crosstab(y_test, pred_test, rownames=['Predicted'], colnames=['Reality'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"788a8bb4cbcb2c97020feaff30be987e9aeef1d6"},"cell_type":"code","source":"data1 = data[data.columns[:9]]\noutcome = data['pred_attribute']\n\n#train,test=train_test_split(dataset,test_size=0.25,random_state=0,stratify=dataset['pred_attribute'])# stratify the outcome\n#train_X=train[train.columns[:13]]\n#test_X=test[test.columns[:13]]\n#train_Y=train['pred_attribute']\n#test_Y=test['pred_attribute']\n\ndata_copy=[]\nfor model in algorithms:\n    model.fit(X_train,y_train)\n    pred_test = model.predict(X_test)\n    data_copy.append(metrics.accuracy_score(pred_test, y_test))\n    \nmodels_df = pd.DataFrame(data_copy, index=names)   \nmodels_df.columns=['Accuracy']\nmodels_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54fdfd22b7e236df06acf0975a5779d0a97baaa2"},"cell_type":"code","source":"sns.heatmap(data[data.columns[:10]].corr(),annot=True,cmap='RdYlGn')\nfig=plt.gcf()\nfig.set_size_inches(25,20)\n#plt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fc696a2706b85619f863965024c7b9a46885c67"},"cell_type":"code","source":"#model = ExtraTreesClassifier(n_estimators=100,random_state=random_state)\n#model.fit(X, y)\n#pd.Series(model.feature_importances_, index=data1.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cc1950f8f26511aefbbdf2f9158078a1dcf33ac"},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100,random_state=random_state) # , max_features=(10 ** 0.5))\nmodel.fit(X, y)\npd.Series(model.feature_importances_, index=data1.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87a8be58105c54c6c7f123484a12e47c6d9f99fb"},"cell_type":"code","source":"data_select = data[['age','sex','fbs','thalach','cp','exang','pred_attribute']]\n\nX = data_select.iloc[:, :-1].values  \ny = data_select.iloc[:, -1].values\n#X = np_dataset[:, :-1]  \n#y = np_dataset[:, -1]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd17cdeafd1104d9cb3b336fa32c9ad4e1c4cf86"},"cell_type":"code","source":"my_imputer = my_imputer.fit(X[:,0:7])   \nX[:, 0:7] = my_imputer.transform(X[:, 0:7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee4176cfaec3b6c43be3f9ff73187114501d7328"},"cell_type":"code","source":"X = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78526c0ff52a78c0497d8094131e9cfa46b4ae3e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b6398c0b121d46e31775d77d20b7bb3a8cb4200"},"cell_type":"code","source":"err_score(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cae80460ec8ace117da0d7fa0c0de18897c7df65"},"cell_type":"code","source":"data_copy=[]\nfor model in algorithms:\n    model.fit(X_train,y_train)\n    pred_test = model.predict(X_test)\n    data_copy.append(metrics.accuracy_score(pred_test, y_test))\n    \nmodels_df2 = pd.DataFrame(data_copy, index=names)  \nmodels_df2.columns = ['New Accuracy']  \n\nmodels_df2 = models_df2.merge(models_df, left_index=True, right_index=True, how='left')\nmodels_df2['Increase'] = models_df2['New Accuracy'] - models_df2['Accuracy']\nmodels_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca6c5a8ac25b9eb1d6aaf94e007cdd8cef6806f9"},"cell_type":"code","source":"kfold = KFold(n_splits=10, random_state=random_state) # k=10, split the data into 10 equal parts\nCV_results = []\n# iterate over classifiers\nAcc= {}\nAcc_Train = {}\nAcc_Test = {}\nStd_Train={}\nStd_Test={}\nPredictions = {}\nROC = {}\nAUC = {}\nGmean = {}\nPrecision = {}\nRecall = {}\nF1_score = {}\nConfusion_Matrix = {}\nmats = pd.DataFrame(Confusion_Matrix)\n\nfor clf in classifiers:\n    #Acc[clf] = cross_validate(classifiers[clf],X=X,y=y,cv=10,n_jobs=-1,scoring='accuracy',return_train_score=True)\n    Acc[clf] = cross_validate(classifiers[clf],X_train,y_train,cv=kfold,n_jobs=-1,scoring='accuracy',return_train_score=True)\n    Acc_Train[clf] =  Acc[clf]['train_score'].mean()\n    Acc_Test[clf] = Acc[clf]['test_score'].mean()\n    Std_Train[clf] = Acc[clf]['train_score'].std()\n    Std_Test[clf] = Acc[clf]['test_score'].std()\n    CV_results.append(Acc[clf])\n    \n    classifiers[clf].fit(scaler.transform(X_train),y_train)\n    pred =  classifiers[clf].predict(scaler.transform(X_test))\n    ROC[clf] = roc_auc_score(y_test,pred)\n    AUC[clf] = auc(y_test,pred,reorder=True)\n    Gmean[clf] = fowlkes_mallows_score(y_test,pred)\n    Precision[clf] = precision_score(y_test,pred)\n    Recall[clf] = recall_score(y_test,pred)    \n    F1_score[clf] = f1_score(y_test,pred)\n    Confusion_Matrix[clf] = confusion_matrix(y_test,pred)\n\nAccuracy_train = pd.DataFrame([Acc_Train[vals]*100 for vals in Acc_Train],columns=['Accuracy Train'],index=[vals for vals in Acc_Train])\nStd_train = pd.DataFrame([Std_Train[vals]*100 for vals in Std_Train],columns=['S. Deviation Train'],index=[vals for vals in Std_Train])\nAccuracy_pred = pd.DataFrame([Acc_Test[vals]*100 for vals in Acc_Test],columns=['Accuracy Test'],index=[vals for vals in Acc_Test])\nStd_test = pd.DataFrame([Std_Test[vals]*100 for vals in Std_Test],columns=['S. Deviation Test'], index=[vals for vals in Std_Test])\nROC_Area = pd.DataFrame([ROC[vals] for vals in ROC],columns=['ROC(area)'],index=[vals for vals in ROC])\nAUC_Area = pd.DataFrame([AUC[vals] for vals in AUC],columns=['AUC(area)'],index=[vals for vals in AUC])\nGm = pd.DataFrame([Gmean[vals] for vals in Gmean],columns=['Gmean'],index=[vals for vals in Gmean])\nPrec = pd.DataFrame([Precision[vals] for vals in Precision],columns=['Precision'],index=[vals for vals in Precision])\nRec = pd.DataFrame([Recall[vals] for vals in Recall],columns=['Recall'],index=[vals for vals in Recall])\nF1 =  pd.DataFrame([F1_score[vals] for vals in F1_score],columns=['F1_score'],index=[vals for vals in F1_score])\n\ntable = pd.concat([Accuracy_train,Std_train,Accuracy_pred,Std_test,ROC_Area,AUC_Area,Gm,Prec,Rec,F1], axis=1)\ntable.loc['MEAN VALUE'] = table.mean()\ntable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2b1e674095631380b0c5810f18aedc6257ab61"},"cell_type":"code","source":"k_range=list(range(1,60))\ndata_copy=pd.Series()\nk_scores = []\n#x=[0,1,2,3,4]\nfor i in k_range:\n    model=KNeighborsClassifier(n_neighbors=i) \n    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n    k_scores.append(scores.mean())\n    model.fit(X_train,y_train)\n    pred_test=model.predict(X_test)\n    data_copy=data_copy.append(pd.Series(metrics.accuracy_score(pred_test,y_test)))\n\n#plt.plot(k_range, k_scores)\nplt.plot(k_range, data_copy)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')    \n#plt.xticks(X)\nplt.show()\nprint('Accuracies for different values of n are:\\n', data_copy.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b18ec9d0e51fea9436fee8b6bbe701c403d2265a"},"cell_type":"code","source":"'''svc = SVC(probability=True, random_state=random_state)\n# Set the parameters by cross-validation\nsvc_pg = [{'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],'C': [1, 10, 100, 1000]},\n          {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    svc.fit(X_train, y_train)\n    y_eval = svc.predict(X_test)\n    acc = sum(y_eval == y_test) / float(len(y_test))\n    print(\"Accuracy of SVC: %.2f%%\" % (100*acc))\n    \n    print(\"Tuning hyper-parameters for %s\\n\" % score)\n    svc_gscv = GridSearchCV(svc, svc_pg, cv=kfold, scoring='%s_macro' % score)\n    svc_gscv.fit(X_train, y_train)\n    print(\"Best parameters set found on training set:\")\n    print(svc_gscv.best_params_,\"\\n\")\n    print(\"Grid scores on training set:\")\n    means = svc_gscv.cv_results_['mean_test_score']\n    stds = svc_gscv.cv_results_['std_test_score']\n    \n    for mean, std, params in zip(means, stds, svc_gscv.cv_results_['params']):\n        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n    print(\"Detailed classification report:\")\n    print(\"The model is trained on the full training set.\")\n    print(\"The scores are computed on the full test set.\")\n    y_true, y_pred = y_test, svc_gscv.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    \n    svc_est = svc_gscv.best_estimator_\n    svc_score = svc_gscv.best_score_\n    print(\"Best estimator for parameter C: %f\\n\" % (svc_est.C))\n    print(\"Best score: %0.2f%%\\n\" % (100*svc_score))\n\n    #print(clf)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42badd74cbc7db6cbf4480f74da809bf55f58a58"},"cell_type":"code","source":"gnb = GaussianNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bfd48b369ad27c7796eec90ec80228e92e1c32b"},"cell_type":"code","source":"mlp = MLPClassifier(momentum=0.15,solver='sgd',learning_rate_init=1.0, early_stopping=True, shuffle=True,random_state=random_state)\n\nmlp_pg={\n'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n'hidden_layer_sizes': [(10,10),(100,),(100,10)],\n#'hidden_layer_sizes': [x for x in itertools.product((10,20,30,40,50,100),repeat=3)],\n#'tol': [1e-2, 1e-3, 1e-4],\n#'epsilon': [1e-3, 1e-7, 1e-8],\n'alpha': [1e-2, 1e-3, 1e-4],\n#'activation': [\"logistic\", \"relu\", \"Tanh\"]\n}\n\nmlp_gscv = GridSearchCV(mlp,param_grid=mlp_pg, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\nmlp_gscv.fit(X_train, y_train)\n\nmlp_est = mlp_gscv.best_estimator_\nmlp_score = mlp_gscv.best_score_\nprint(\"Best estimator:\", mlp_est, \"\\nBest Score:\", mlp_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d805902e69f3a932ed51829892464b5e2eb6da"},"cell_type":"code","source":"lr = LogisticRegression(\n    C=0.1,\n    penalty='l2',\n    dual=True,\n    tol=0.0001, \n    fit_intercept=True,\n    intercept_scaling=1.0, \n    class_weight=None,\n    random_state=43)\n\nlr_pg = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nlr_gscv = GridSearchCV(lr,param_grid=lr_pg, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\nlr_gscv.fit(X_train, y_train)\n\nlr_est = lr_gscv.best_estimator_\nlr_score = lr_gscv.best_score_\nprint(\"Best estimator:\", lr_est, \"\\nBest Score:\", lr_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fddfd05db3cfda35ee0408b2c5f1124533b5d58"},"cell_type":"code","source":"predictor1=LogisticRegression(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1.0, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=43, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False).fit(X_train, y_train)\nprediction=predictor1.predict(X_test)\ntest_healthy = pd.Series(prediction, name=\"Healthy\")\npredictor1.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9582f72ac98c25481d3582fc8d16f36fbbe6fb68"},"cell_type":"code","source":"submission = pd.DataFrame({'Healthy': prediction })\nsubmission.to_csv(\"Results_df_NSR.csv\", index=True)\nprint(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1cf9245bb72f79113edf9c0f97606673df61368"},"cell_type":"code","source":"xgbc = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n     n_estimators= 2000,\n     max_depth= 4,\n     min_child_weight= 2,\n     gamma=0.9,                        \n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     nthread= -1,\n     scale_pos_weight=1)\n\nxgbc.fit(X_train, y_train)\n\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train, test_size=0.3, random_state=random_state)\ntrain2 = xgb.DMatrix(X_train2, y_train2)\ntest2 = xgb.DMatrix(X_test2, y_test2)\nwatchlist = [(test2, 'test')]\nxgb_pg = {\n    'max_depth': 3,\n    'booster': 'gbtree',\n    'objective': 'reg:linear',#'binary:logistic'\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'eta': 0.05,\n    'max_depth': 7,\n    'seed': 43,\n    'silent': 0,\n    'eval_metric': 'rmse' #root mean square estimate\n}\n\nbst = xgb.train(xgb_pg, train2, 500, watchlist, early_stopping_rounds=50)\n#bst.save_model(\"model_new\")\n#bst = xgb.Booster(xgb_pg)\n#bst.load_model(\"model_new\")\n\n# make prediction\n#pred = bst.predict(xgb.DMatrix(X_test))\nprediction = xgbc.predict(X_test)\npred = bst.predict(test2,ntree_limit=bst.best_ntree_limit)\nxgb.to_graphviz(bst, num_trees=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'Healthy': prediction })\nsubmission.to_csv(\"Results_df_NSR.csv\", index=True)\nprint(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa47bf1cd8496543320ac4918c059b340e51c966"},"cell_type":"code","source":"class Ensemble(object):\n    def __init__(self, n_folds, stacker, base_models):\n        self.n_folds = n_folds\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n\n        for i, clf in enumerate(self.base_models):\n            S_test_i = np.zeros((T.shape[0], len(folds)))\n\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n                # y_holdout = y[test_idx]\n                clf.fit(X_train, y_train)\n                y_pred = clf.predict(X_holdout)[:]\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict(T)[:]\n\n            S_test[:, i] = S_test_i.mean(1)\n\n        self.stacker.fit(S_train, y)\n        y_pred = self.stacker.predict(S_test)[:]\n        return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34831ffdc2530f0517317d87224cca677f5c472f"},"cell_type":"code","source":"gnb_mlp=VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Multilayer Perceptron',mlp)], voting='soft', weights=[2,1]).fit(X_train,y_train)\nprint('The accuracy for Guassian Naive Bayes and Multilayer Perceptron:',gnb_mlp.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"808b490621e7bbf76eb0d269f6b19fc71a4e7ad0"},"cell_type":"code","source":"gnb_lr=VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Logistic Regression', lr)], voting='soft', weights=[2,1]).fit(X_train,y_train)\nprint('The accuracy for Guassian Naive Bayes and Logistic Regression:',gnb_lr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca124d7d8c3e2bb1f7012d77042c510a8dbfa700"},"cell_type":"code","source":"mlp_lr=VotingClassifier(estimators=[('Multilayer Perceptron',mlp), ('Logistic Regression', lr)], voting='soft', weights=[2,1]).fit(X_train,y_train)\nprint('The accuracy for Multilayer Perceptron and Logistic Regression:',mlp_lr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"497aa87389fbd65bf77380050b2e1c1adb1daedd"},"cell_type":"code","source":"gnb_mlp_lr=VotingClassifier(estimators=[('Guassian Naive Bayes', gnb),('Multilayer Perceptron',mlp), ('Logistic Regression', lr)], voting='soft', weights=[3,2,1]).fit(X_train,y_train)\nprint('The ensembled model with Guassian Naive Bayes, Multilayer Perceptron, and Logistic Regression:',gnb_mlp_lr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4765a73e794d19c20dc6a3ffbd836fc4bd485670"},"cell_type":"code","source":"# Generate a simple plot of the test and training learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)): \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\n#gnb_plot = plot_learning_curve(gnb_est,\"GNB learning curves\",X_train,Y_train,cv=kfold)\nmlp_plot = plot_learning_curve(mlp_est,\"MLP learning curves\",X_train,y_train,cv=kfold)\nlr_plot = plot_learning_curve(lr_est,\"LR learning curves\",X_train,y_train,cv=kfold)\nsvc_plot = plot_learning_curve(svc_est,\"SVC learning curves\",X_train,y_train,cv=kfold)\n#xgb_plot = plot_learning_curve(xgb_gscv.best_estimator_,\"XBG learning curves\",X_train,Y_train,cv=kfold)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d378c4f70e3ee111edc98658742f78fdf0844ad"},"cell_type":"code","source":"#test_survived_gnb = pd.Series(gnb_est.predict(test), name=\"GNBC\")\ntest_Survived_mlp = pd.Series(mlp_est.predict(X_test), name=\"MLP\")\ntest_Survived_lr = pd.Series(lr_est.predict(X_test), name=\"LR\")\ntest_Survived_svc = pd.Series(svc_est.predict(X_test), name=\"SVC\")\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_mlp,test_Survived_lr,test_Survived_svc],axis=1)\ng= sns.heatmap(ensemble_results.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3581b41016fc8310f072bfa12026fdb9de9be98d"},"cell_type":"code","source":"finalVC = VotingClassifier(estimators=[('Logistic Regression', lr),('SVM',svc)], voting='soft', weights=[2,1], n_jobs = -1).fit(X_train,y_train)\nprint('The ensembled model with all classfiers: ',finalVC.score(X_test,y_test))\n\n# Generate Submission File \ntest_healthy = pd.Series(finalVC.predict(X_test), name=\"Healthy\")\n#results = pd.concat([X_test['index'],test_healthy],axis=1)\n#results.to_csv(\"Results_series_NSR.csv\", index=False)\n#submission1 = pd.DataFrame({'Healthy Probabilty': pred })\n#submission1.to_csv(\"Results_df1_NSR.csv\", index=True)\n#print(submission1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452bac45e4124fad4e1e8389d167a51d80fb9ac7"},"cell_type":"code","source":"submission = pd.DataFrame({'Healthy': prediction })\nsubmission.to_csv(\"Results_df_NSR.csv\", index=True)\nprint(submission)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}